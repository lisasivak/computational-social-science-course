{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with Python\n",
    "\n",
    "This is a short tutorial on web scraping. \n",
    "\n",
    "Web scraping is the automated extraction of data from websites. All websites are written using text-based markup languages (HTML and XHTML), and each page is essentially a code with embedded pieces of data (e.g. texts, photos, links to other pages, etc.) that we want to get as a result of scraping. The browser translates the code into what we see on the site as users. To get the data we need, we first need to get the page code, parse it, and then extract the necessary data.\n",
    "\n",
    "Parsing the page code - or determining which tags and attributes contain the site elements that we need, for example, comments from users of a site - may not be very easy. Sometimes there is different data under the same tags and attributes, and it is not very clear how to automatically collect only the data we need. In addition, the page code can change - and you have to invent everything anew. Some resources, such as YouTube or Vkontakte, provide everyone with a list of commands for collecting the necessary data (application programming interface, API). This is much more convenient - there is a list of commands, and we do not have to figure out how to get what we need. However, not all resources on the Internet have an API. Therefore, web scraping can be a useful tool.\n",
    "\n",
    "An important rule: before collecting information from the site, find the terms of use and make sure that scraping is not prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how web scraping works, let's download the transcripts of \"The Big Bang Theory\" series from here https://bigbangtrans.wordpress.com (kudos to the creator of this site!). \n",
    "\n",
    "Let's try to see the code of the page with the transcript of the first episode https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/. You need to press Ctrl + Shift + I or the right mouse button -> press \"see the code\". If you click on the arrow icon in the upper left corner of the code window, you can hover over different elements on the page and see where they are in the form of code, under which tags they are stored.\n",
    "\n",
    "You can also slowly move the mouse over the lines of page code on the right and see which page fragments are highlighted. And thus look for a code fragment that contains the data we need.\n",
    "\n",
    "We can see, for example, that links are written with the <\\a> tag - in html, this is a command to the browser that the text contained in the tag is a web link. You can read more about html tags, for example, here http://htmlbook.ru/\n",
    "\n",
    "We are interested in the transcript of the series itself - it is contained under the \"div\" tag with the atribute 'class=\"entrytext\"'\n",
    "\n",
    "Now let's try to extract this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first stage is getting the page code, which we will then parse\n",
    "# To get the content of web pages, the \"requests\" package is used\n",
    "# let's import it\n",
    "\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can use this package to send requests and receive information from web pages\n",
    "# We can read the package documentation and find required command https://docs.python-requests.org/en/master/\n",
    "\n",
    "# Let's create a variable with the link to the page we need to get\n",
    "\n",
    "url = \"https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/\"  \n",
    "\n",
    "# creating a request to get this page, passing the link as a parameter:\n",
    "\n",
    "response = requests.get(url) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sometimes a site requires details about the user agent, i.e. the browser from which the request is made*   \n",
    "*In this case, you can specify any data, for example these*\n",
    "\n",
    "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\\n\\n<head profile=\"http://gmpg.org/xfn/11\">\\n\\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\n\\t<title>Series 01 Episode 01 &#8211; Pilot Episode | Big Bang Theory Transcripts</title>\\n\\t<!--[if lte IE 8]>\\n\\t<link rel=\"stylesheet\" href=\"https://s2.wp.com/wp-content/themes/pub/chaoticsoul/ie.css\" type=\"text/css\" media=\"screen\" />\\n\\t<![endif]-->\\n\\t<link rel=\"pingback\" href=\"https://bigbangtrans.wordpress.com/xmlrpc.php\" />\\n\\t<meta name=\\'robots\\' content=\\'max-image-preview:large\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//s2.wp.com\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//s1.wp.com\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//s0.wp.com\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//s.pubmine.com\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//x.bidswitch.net\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//static.criteo.net\\' />\\n<link rel=\\'dns-prefetch\\' href'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the entire content of the page is written to the response variable\n",
    "# we can see the entire content with the command response.text. Let's see some of it\n",
    "\n",
    "response.text[0:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text looks really complicated. We need a parser - a set of commands with which you can separate the code (tags, attributes) from everything else, and get the necessary data.\n",
    "\n",
    "We will use the html parser from the BeautifulSoup package. Documentation -https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://bigbangtrans.wordpress.com/\">Big Bang Theory Transcripts</a>,\n",
       " <a class=\"share-twitter sd-button share-icon\" data-shared=\"sharing-twitter-3\" href=\"https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Twitter\"><span>Twitter</span></a>,\n",
       " <a class=\"share-facebook sd-button share-icon\" data-shared=\"sharing-facebook-3\" href=\"https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Facebook\"><span>Facebook</span></a>,\n",
       " <a class=\"sd-link-color\"></a>,\n",
       " <a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=4682827\"><img src=\"https://www.paypal.com/en_GB/i/btn/btn_donate_SM.gif\"/></a>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's parse the page\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "# The same page is now written to the \"soup\" variable, but in a more structured form\n",
    "# knowing by tags, we can get what we need using the soup.findAll(tag) method\n",
    "# for example, let's find all the links using the tag \"a\"\n",
    "\n",
    "# we still have some pieces of code, but what we get is much more comprehensible \n",
    "\n",
    "all_links = soup.findAll('a')\n",
    "all_links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's finally get the transcripts\n",
    "# If you view the page code in the browser, we will find out that the text we need is contained under the \"div\" tag with the attribute 'class = \"entrytext\"'\n",
    "\n",
    "# *by the way, this is the hardest part - to understand which tags\\attributes do you need. It takes time and experimentation*\n",
    "# \n",
    "\n",
    "# So, let's use this tag\\attribute to construct the command\n",
    "# We learned the structure of the command from the BeautifulSoup documentation (in case you wonder)\n",
    "\n",
    "soup.findAll('div', {'class': 'entrytext'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a list-like structure. Now we need to get the text and get rid of the code. We learned from the documentation that this can be done using the \"get_text()\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScene: A corridor at a sperm bank.\\nSheldon: So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.\\nLeonard: Agreed, what’s your point?\\nSheldon: There’s no point, I just think it’s a good idea for a tee-shirt. \\nLeonard: Excuse me?\\nReceptionist: Hang on. \\nLeonard: One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti. \\nReceptionist: Can I help you?\\nLeonard: Yes. Um, is this the High IQ sperm bank?\\nReceptionist: If you have to ask, maybe you shouldn’t be here.\\nSheldon: I think this is the place.\\nReceptionist: Fill these out.\\nLeonard: Thank-you. We’ll be right back.\\nReceptionist: Oh, take your time. I’ll ju'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = soup.findAll('div', {'class': 'entrytext'})[0].get_text()\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we got almost what we need. However, if you look at the whole transcript you will see that at the end there are some technical symbols. Let's remove them. This can be done in many ways, but we will use regular expressions (https://docs.python.org/3/howto/regex.html) because on different pages of this site, this text probably looks a little different. We need to remove the part which starts with \"\\__ATA\" and ends with \":Like Loading...\". The regular expressioin for that is `\"__ATA(.|\\s)*:Like Loading...\"`\n",
    "\n",
    "`(.|\\s)*` means any symbol `(.)` or end of paragraph `(.|\\s)` repeated any number of times `*`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scene: A corridor at a sperm bank.',\n",
       " 'Sheldon: So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
       " 'Leonard: Agreed, what’s your point?',\n",
       " 'Sheldon: There’s no point, I just think it’s a good idea for a tee-shirt. ',\n",
       " 'Leonard: Excuse me?',\n",
       " 'Receptionist: Hang on. ',\n",
       " 'Leonard: One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti. ',\n",
       " 'Receptionist: Can I help you?',\n",
       " 'Leonard: Yes. Um, is this the High IQ sperm bank?',\n",
       " 'Receptionist: If you have to ask, maybe you shouldn’t be here.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing regular expressions\n",
    "import re\n",
    " \n",
    "# Deleting the technical text\n",
    "lines = re.sub(\"__ATA(.|\\s)*:Like Loading...\",\"\", text)\n",
    "\n",
    "# Splitting the whole text into lines\n",
    "lines = lines.split('\\n')\n",
    "lines\n",
    "\n",
    "# Removing empty lines\n",
    "lines = [x for x in lines if x != \"\"]\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now save the result into .txt\n",
    "with open(\"s1e1.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    for i in lines:\n",
    "        text_file.write(i + '\\n') # Добавляем знак конца строки, чтобы у нас все реплики были на отдельной строке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded one transcript. But we need all of them. To do this we need to somehow generate or get all the links to the pages. There is no universal method here. Sometimes links look like \"https://myblog.com/?skip=0\", \"https://myblog.com/?skip=10\", \"https://something.com/?skip=20, etc. In this case, we can generate the links by changing only the number at the end. In our current case, the links look different - they include the name of an episode, so we cannot generate them straight away. But each page with a transcript has a \"Pages\" section on the left with a list of all the eposodes' names with links to them. We can gather the links to request the pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the <\\a> tag which is used for storing links. The link destination (URL) itself is stored with the \"href\" attribute (one can find this out by using any tutorial about the html language, for example, here http://htmlbook.ru/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://bigbangtrans.wordpress.com/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/?share=twitter',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/?share=facebook',\n",
       " 'https://bigbangtrans.wordpress.com/about/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-2-the-big-bran-hypothesis/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-3-the-fuzzy-boots-corollary/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-4-the-luminous-fish-effect/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-5-the-hamburger-postulate/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-6-the-middle-earth-paradigm/']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_with_text = []\n",
    "\n",
    "links = soup.findAll('a', href=True)\n",
    "for link in links:\n",
    "    # Let's add an additional condition if the link has some displayed text (as in the Pages section), because there can be many different links on a page.\n",
    "    if link.text: \n",
    "        links_with_text.append(link['href'])\n",
    "            \n",
    "\n",
    "links_with_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-2-the-big-bran-hypothesis/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-3-the-fuzzy-boots-corollary/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-4-the-luminous-fish-effect/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-5-the-hamburger-postulate/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-6-the-middle-earth-paradigm/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-7-the-dumpling-paradox/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-8-the-grasshopper-experiment/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-9-the-cooper-hofstadter-polarization/',\n",
       " 'https://bigbangtrans.wordpress.com/series-1-episode-10-the-loobenfeld-decay/']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the links that we don't need\n",
    "links_with_text = [link for link in links_with_text if \"https://bigbangtrans.wordpress.com/series\" in link and \"share\" not in link]\n",
    "links_with_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['series-1-episode-1-pilot-episode',\n",
       " 'series-1-episode-2-the-big-bran-hypothesis',\n",
       " 'series-1-episode-3-the-fuzzy-boots-corollary',\n",
       " 'series-1-episode-4-the-luminous-fish-effect',\n",
       " 'series-1-episode-5-the-hamburger-postulate',\n",
       " 'series-1-episode-6-the-middle-earth-paradigm',\n",
       " 'series-1-episode-7-the-dumpling-paradox',\n",
       " 'series-1-episode-8-the-grasshopper-experiment',\n",
       " 'series-1-episode-9-the-cooper-hofstadter-polarization',\n",
       " 'series-1-episode-10-the-loobenfeld-decay']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the part of the link containing the season/episode's number and episode's name to save files under these names\n",
    "\n",
    "titles = []\n",
    "\n",
    "for i in range(len(links_with_text)):\n",
    "    a=links_with_text[i].replace('https://bigbangtrans.wordpress.com/',\"\")\n",
    "    a=a.rstrip(\"/\")\n",
    "    titles.append(a)\n",
    "\n",
    "titles[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now everything is ready to scrape the pages\n",
    "\n",
    "for i, url in enumerate(links_with_text):\n",
    "    #print(url) # we can print the link to see where the problem is if something doesn' work\n",
    "    response = requests.get(url,headers={\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    text = soup.findAll('div', {'class': 'entrytext'})[0].get_text()       \n",
    "\n",
    "    lines = re.sub(\"__ATA(.|\\s)*:Like Loading...\", \"\", text)\n",
    "    lines = lines.split(\"\\n\")\n",
    "    lines = [x for x in lines if x != \"\"]  \n",
    "    \n",
    "    # saving the contents into a .txt файл\n",
    "    with open(titles[i]+\".txt\", \"w\", encoding=\"utf-8\") as text_file: \n",
    "        for i in lines:\n",
    "            text_file.write(i + '\\n') \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
